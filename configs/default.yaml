base:
  data_path: /path/to/your/dataset
  data_index: null # alternative way to build dataset. check README for more details
  save_path: ./run/experiment # path to save
  device: cuda
  random_seed: 0 # set to -1 to disable random seed
  cudnn_deterministic: false # set to True to turn on CUDNN deterministic setting, but it may slow down your training
  overwrite: false # overwrite save_path
  progress: true # real-time metric display, output cannot be redirected
  HPO: false # hyper-parameter optimization using nni. ref: https://github.com/microsoft/nni

dist:
  distributed: false # distributed data parallel (DDP) model for multiple gpus training
  backend: nccl # default backend for DDP
  nodes: 1 # number of nodes for distributed training 
  n_gpus: null # use all visiable gpus when n_gpus is set to null
  addr: '127.0.0.1' # address used to set up distributed training
  port: '29500' # port used to set up distributed training
  rank: 0 # node rank for distributed training

data:
  num_classes: 1000 # number of classes
  input_size: 224 # image size
  in_channels: 3 # number of image channel
  mean: auto # 'auto' or a list of three numbers for RGB
  std: auto # 'auto' or a list of three numbers for RGB
  sampling_strategy: instance_balanced # instance_balanced / class_balanced / progressively_balanced. ref: https://arxiv.org/abs/1910.09217
  sampling_weights_decay_rate: 0.9 # if sampling_strategy is progressively_balanced, sampling weight will change from class_balanced to instance_balanced
  data_augmentation: # available operations are list in 'data_augmentation_args' below
    - random_crop
    - horizontal_flip
    - vertical_flip
    - color_distortion
    - rotation
    - translation

train:
  network: resnet50 # available networks are list in networks.yaml
  backend: timm # network builder backend (timm or torchvision)
  pretrained: true # load weights from pre-trained model training on ImageNet
  checkpoint: null # load weights from other pretrained model
  epochs: &epochs 50
  batch_size: 32
  num_workers: 8 # number of cpus used to load data at each step
  criterion: cross_entropy # available criterions are list in 'criterion_args' below
  loss_weight: null # null / balance / dynamic / list with shape num_classes. Weights for loss function. Don't use it with weighted sampling!
  loss_weight_decay_rate: 0 # if loss_weights is dynamic, loss weight will decay from balance to equivalent weights
  warmup_epochs: 0 # set to 0 to disable warmup
  metrics: [acc, f1, auc, precision, recall, kappa] # available metrics are list in utils.metrics
  indicator: kappa # indicator for best model selection in validation set
  save_interval: 5 # the epoch interval of saving model
  eval_interval: 1 # the epoch interval of evaluating model on val dataset
  sample_view: false # save and visualize a batch of images on Tensorboard
  pin_memory: true # enables fast data transfer to CUDA-enabled GPUs

solver:
  optimizer: SGD # SGD / ADAM / ADAMW
  learning_rate: 0.001 # initial learning rate
  lr_scheduler: cosine # available schedulers are list in 'scheduler_args' below. please remember to update scheduler_args when number of epoch changes.
  momentum: 0.9 # only for SGD. set to 0 to disable momentum
  nesterov: true # only for SGD.
  weight_decay: 0.0005 # set to 0 to disable weight decay
  adamw_betas: [0.9, 0.999] # for ADAMW optimizer

criterion_args:
  cross_entropy: {}
  mean_square_error: {}
  mean_absolute_error: {}
  smooth_L1: {}
  kappa_loss: {}
  focal_loss:
    alpha: 5
    reduction: mean

# please refer to documents of torch.optim
scheduler_args:
  exponential:
    gamma: 0.6 # multiplicative factor of learning rate decay
  multiple_steps:
    milestones: [15, 25, 45]
    gamma: 0.1 # multiplicative factor of learning rate decay
  cosine:
    T_max: *epochs # maximum number of iterations
    eta_min: 0 # minimum learning rate
  reduce_on_plateau:
    mode: min
    factor: 0.1 # new learning rate = factor * learning rate
    patience: 5 # number of epochs with no improvement after which learning rate will be reduced.
    threshold: 0.0001 # threshold for measuring the new optimum
    eps: 0.00001 # minimal decay applied to learning rate
  clipped_cosine:
    T_max: *epochs
    min_lr: 0.0001

data_augmentation_args:
  horizontal_flip:
    prob: 0.5
  vertical_flip:
    prob: 0.5
  color_distortion:
    prob: 0.5
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1
  random_crop: # randomly crop and resize to input_size
    prob: 0.5
    scale: [0.87, 1.15] # range of size of the origin size cropped
    ratio: [0.7, 1.3] # range of aspect ratio of the origin aspect ratio cropped
  rotation:
    prob: 0.5
    degrees: [-180, 180]
  translation:
    prob: 0.5
    range: [0.2, 0.2]
  grayscale: # randomly convert image to grayscale
    prob: 0.5
  gaussian_blur: # only available for torch version >= 1.7.1.
    prob: 0.2
    kernel_size: 7
    sigma: 0.5
  value_fill: 0 # NOT a data augmentation operation. pixel fill value for the area outside the image

config_check:
  cosine_decay_epochs: true # check if the epoch of cosine decay is same as training epochs